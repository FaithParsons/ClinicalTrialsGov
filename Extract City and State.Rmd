---
title: "Descriptives and Stats"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(readr)
library(janitor)
library(RCurl)
library(RJSONIO)
library(datasets)
library(stringr)

results = read_csv("Data/SearchResultsTable.csv") %>%
  clean_names()  

# Determine number of variables to create for location
nlocs = str_count(results$locations , "\\|") %>%
  max(na.rm = TRUE) + 1

# Determine number of variables to create for interventions
ntreatment = str_count(results$interventions, "\\|") %>%
  max(na.rm = TRUE) + 1

# Extract locations
locations = results %>%
  # Only keep nct_number and locations variable
  select(nct_number, locations) %>%
  # Separate locations into multiple columns
  separate(locations, into = paste("loc", 1:nlocs, sep = ""), sep = "\\|", fill = "right") %>%
  # Convert to longitudinal format
  gather(key = locnum, value = locname, loc1:paste("loc", nlocs, sep = "")) %>%
  # remove if locname = NA
  filter(!is.na(locname)) %>%
  # Sort by nct_number
  arrange(nct_number)

# How many commas in each locname? This will determine number of variables to split each location into
ncomma = str_count(locations$locname, ",") %>%
  max(na.rm = TRUE) + 1

# Split locnames to identify city, state and country; Eventually only keep United States locations
loc_states = locations %>%
  separate(locname, into = paste("subloc", 1:ncomma, sep=""), sep = ",", fill = "right") %>%
  # Only keep if in United States
  filter(str_detect(subloc1, "United States") | str_detect(subloc2, "United States") |
           str_detect(subloc3, "United States") | str_detect(subloc4, "United States") |
           str_detect(subloc5, "United States") | str_detect(subloc6, "United States") |
           str_detect(subloc7, "United States") | str_detect(subloc8, "United States") |
           str_detect(subloc9, "United States")) %>%
  #Convert to longitudinal so we can extract the City and State
  gather(key = sublocnum, value = sublocname, subloc1:subloc9) %>%
  arrange(nct_number, locnum, sublocnum) %>%
  # Remove blank sublocnames, and remove rows containing "United States"
  filter(!is.na(sublocname) & !str_detect(sublocname, "United States")) %>%
  # Convert sublocnum to numeric
  mutate(sublocnum = as.numeric(gsub("[^0-9]", "", sublocnum)),
         locnum = as.numeric(gsub("[^0-9]", "", locnum))) %>%
  # Sort sublocnum in descending order so that State is at the top, and City next
  arrange(nct_number, locnum, desc(sublocnum)) %>%
  group_by(nct_number, locnum) %>%
  # Reverse code sublocnum
  mutate(sublocnum = max(sublocnum) - sublocnum +1)  %>%
  ungroup()

#View(loc_states)

# Create separate City & State variables
loc_city_state = loc_states %>%
  # Convert to wide format, 1 row per location (we still have multiple rows per NCT_ID)
  spread(sublocnum, sublocname) %>%
  clean_names() %>%
  # Combine x3 onwards into one variable
  rename(state = x1,
         city = x2) %>%
  tidyr::unite(institution, x3:x7, sep = ",") %>%
  # Remove ",NA" from inst
  mutate(institution = gsub(",NA", "", institution))

saveRDS(loc_city_state, "Data/loc_city_state")
# Number of US Sites per study

num_ussites = loc_city_state %>%
  group_by(nct_number) %>%
  summarize(num_ussites = n())

# Save File to data folder
saveRDS(num_ussites, file="Data/num_ussites.rds")

#View(loc_city_state)  

#ftable(loc_city_state$city) %>% as.tibble() %>% View()
```

```{r geocode function}

# This is a function someone wrote that doesn't require google's API
# Source: https://stackoverflow.com/questions/3257441/geocoding-in-r-with-google-maps


construct.geocode.url <- function(address, return.call = "json", sensor = "false") {
  root <- "http://maps.google.com/maps/api/geocode/"
  u <- paste(root, return.call, "?address=", address, "&sensor=", sensor, sep = "")
  return(URLencode(u))
}

gGeoCode <- function(address,verbose=FALSE) {
  if(verbose) cat(address,"\n")
  u <- construct.geocode.url(address)
  doc <- getURL(u)
  x <- fromJSON(doc,simplify = FALSE)
  if(x$status=="OK") {
    lat <- x$results[[1]]$geometry$location$lat
    lng <- x$results[[1]]$geometry$location$lng
    #return(c(lat, lng))
    #return(cbind(lat, lng, address))
    geocoded = cbind(lat, lng, address) %>% as.tibble()
  } else {
    return(c(NA,NA))
  }
}

x = gGeoCode("New York, NY")
str(x)
```

```{r geocode locations}

# Read the RDS file
locs = readRDS("Data/loc_city_state") %>%
  mutate(state = str_trim(state))


# Create state name and abbrev. lookup
state_list <- list()
state_list[['name']] = c(state.name,"District of Columbia")
state_list[['abb']] = c(state.abb,"DC")

state_lookup <- setNames(state_list$abb, state_list$name)
##filter out any entry without state because can't map it  
state_coded <- locs %>% 
  rename(state_name = state) %>%
  filter(!is.na(state_name)) %>%
  mutate(state_code = state_lookup[state_name],
         location = paste(city, state_code, sep = ", "), 
         location = str_trim(location))

locations = state_coded %>%
  select(location) %>%
  unique() %>%
  pull(location)

#GEocode all locations

loctest = locations[1:2] 

test = map(locations, gGeoCode)
geocoded = bind_rows(test) %>%
  rename(location = address)



geocoded_studies = left_join(state_coded, geocoded, by = "location", match = "all") 

# Save permanently

saveRDS(geocoded_studies, "Data/geocoded_studies.rds")
```



